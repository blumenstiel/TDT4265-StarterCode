{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Report\n",
    "\n",
    "Group 121 - Etienne Gaucher and Benedikt Blumenstiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "\"Intersection over Union\" is a measurement to calculate the overlapping of two boundaries (like bounding boxes of\n",
    "predictions or the ground through). We can use the IoU to identify objects that are detected multiple times (non maximum\n",
    "supression) as well as to determine if a detection is correct. To calculate IoU, we first calculate the area of the\n",
    "overlap and divide it by the area of the union of two bounding boxes, as shown in the drawing.\n",
    "\n",
    "![](task1a_iou.png)\n",
    "\n",
    "## task 1b)\n",
    "\n",
    "\\begin{align*}\n",
    "Precision &= \\frac{True Positive}{True Positive + False Positive}\\\\\n",
    "\\\\\n",
    "Recall &= \\frac{True Positive}{True Positive + False Negative}\n",
    "\\end{align*}\n",
    "\n",
    "A true positive is a correct detection of an object (detected bounding box has the correct class and an IoU over a given\n",
    "threshold). A false positive is a wrong detection of an object (IoU under threshold or wrong class).\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "\\begin{align*}\n",
    "AP_1 &= (5 \\times 1.0 + 3 \\times 0.5 + 3 \\times 0.2)/11 = 0.6455\\\\\n",
    "\\\\\n",
    "AP_2 &= (4 \\times 1.0 + 0.8 + 0.6 + 2 \\times 0.5 + 3 \\times 0.2)/11 = 0.6364\\\\\n",
    "\\\\\n",
    "mAP &= (AP_1 + AP_2)/2 = 0.6409\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n",
    "![](task2/precision_recall_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3a)\n",
    "This filtering operation is called non-maximum suppression (nms).\n",
    "\n",
    "### Task 3b)\n",
    "It is false: deeper layers are lower resolution layers. Therefore, SSD uses them to detect large scale objects.\n",
    "On the contrary, higher resolution feature maps are responsible for detecting small objects.\n",
    "\n",
    "### Task 3c)\n",
    "They use different bounding box aspect ratios at the same spatial location because we want our predictions to be diverse and\n",
    "not looking similar. If our predictions are more various and cover more shapes, our model can detect more object types.\n",
    "Also, the fact is in real-life, boundary boxes do not have arbitrary shapes and sizes, so we have to cover more possible\n",
    "cases. For example, the shape of a car and a cyclist is different, so even if they are at the same spatial location on two different pictures, if we only consider one shape\n",
    "of bounding box, we can only detect either the cyclist or the car. Considering only one shape of bounding box detects\n",
    "the objects with this specific shape, but the other objects are not detected because of their different shapes.\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "SSD model adds several convolutional feature layers to the end of a base network. These layers decrease in size\n",
    "progressively and allow predictions of detections at multiple scales and aspect ratios.\n",
    "Instead of adding convolutional layers, YOLO operates on a single scale feature map, it uses an intermediate fully\n",
    "connected layer. These two models do not use the same type of layers after the base network.\n",
    "\n",
    "### Task 3e)\n",
    "We have in total $38 \\times 38 \\times 6 = 8664$ anchors boxes for this feature map.\n",
    "\n",
    "### Task 3f)\n",
    "We have in total $(38 \\times 38 + 19 \\times 19 + 10 \\times 10 + 5 \\times 5 + 3 \\times 3 + 1 \\times 1) \\times 6 = 11 640$\n",
    "anchors boxes for the entire network.\n",
    "\n",
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 4b)\n",
    "\n",
    "![](task4b_total_loss.JPG)\n",
    "\n",
    "The mean average precision (mAP) is 0,7670."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 4c)\n",
    "\n",
    "The final mAP is equal to 0,867.\n",
    "To reach this accuracy, we first add batch normalization after each convolutional layer. Then, we add one more\n",
    "convolutional layer between each feature map except from the first output (so we add 5 convolutional layers).\n",
    "Moreover, we change the number of filters to 256 for all the convolutional layers except for feature outputs.\n",
    "We use the Adam optimizer with a learning rate of 0,0002."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}